# Elasticsearch POC: Gelişmiş Profil Araması

## Problem
Mevcut search, bio içeriğindeki bilgileri (kullanıcının belirttiği okul, takım, mezuniyet yılı vb.) yakalayamıyor. Örnek: "ITÜ '22" araması sonuç vermiyor.

## İş Etkisi
Elasticsearch'ün implementasyonu, kullanıcıların ortak bağlantıları hızla keşfetmesini sağlayabilir, platformdaki etkileşim potansiyelini artırabilir.
Bunun sonucunda aşağıda belirtilen durumlar ile olumlu anlamda karşılaşılabilir.

* Kullanıcılar aynı okul/mezuniyet yılı/takımdan kişileri keşfedebilir
* Konsept dışı farklı noktası olan kişileri bulma kolaylığı
* Superlike benzeri consumable harcamalarında olası artış
* Kullanıcı etkileşiminde potansiyel artış

## Metrikler
Elasticsearch ile gerçekleştirdiğimiz geliştirmeler, uygulamadaki kullanıcı davranışlarını  anlamamıza ve ölçümlememize olanak tanıyabilir.
* Bio bazlı aramaların artışı,
* Arama sonrası kullanıcı etkileşimleri (user'lar arası etkileşim veya consumable harcaması)
* Kullanıcı retention oranları

**_Benzeri metriklerle bu iyileştirmelerin etkisini doğrudan gözlemleyebiliriz._**

Ayrıca, Elasticsearch'ün bize sunduğu **_Kibana Dashboard_** ile
* Veri görselleştirme ve analiz süreçlerini daha verimli hale getirebiliriz.
* Bio keyword trendlerini (örneğin en çok aranan okul veya takımlar)
* Kullanıcıların arama pattern'lerini,
* Başarılı match'lerin bio analizini

**_Benzeri metrikleri takip edebiliriz._**

## Risk & Limitler
1. İlk olarak, **_initial data sync_** süresi beklentilerin üzerinde zaman alabilir ve bu süreçte performans kaybı yaşanabilir.
2. Ayrıca, **_peak saatlerde_** performans düşüşü olasılığı göz önünde bulundurulmalı ve sistemin ölçeklenebilirliği test edilmelidir.
3. Son olarak, yüksek veri hacmi nedeniyle **_storage büyüme hızı_**, uzun vadede maliyet ve yönetim zorlukları doğurabilir.


# Process
Biyografi verilerinin güncellenmesi ve loglanması sürecini açıklayarak Elasticsearch entegrasyonunu şu şekilde ele alabiliriz.

### 1. Biyografi Güncelleme Süreci
Kullanıcı biyografileri, sistemde güncelleme yapıldığında aşağıdaki işlemler gerçekleşir:

* Kullanıcı, bio'sunu set eder ve bu bio limitlere uygunluk açısından kontrol edilir
* Biyografi, PostgreSQL’deki users tablosunda güncellenir.
* Güncelleme bilgisi Redis’e yazılır ve WebSocket aracılığıyla servislere iletilir.
* BigQuery için bir log oluşturulur ve bu log Kafka üzerinden server-user-events topic’ine gönderilir.

### 2. Loglama Süreci

BigQuery’ye gönderilen `server-user-events` topic’inden `user_bio_changed` key’ine sahip mesajlar yakalanır ve PostgreSQL’deki `users_bio_log` tablosuna yazılır. Bu tablo, `user_id`, `bio`, `log_date` column'larına sahiptir.

### 3. Elasticsearch Entegrasyonu

#### Loglanan Verilerin Elasticsearch’e Aktarılması

users_bio_log tablosundaki veriler, Elasticsearch’te bir index oluşturularak kullanılabilir. Bu süreçte

> Index adı: `user_bio_logs`
Fieldlar:
user_id (TEXT)
bio (TEXT)
log_date (TIMESTAMPT)

```go
if *message.Key != "changed_bio" {
    return true
}

userID := *message.ID
bio := *message.Value

_, err = c.userDB.Exec(ctx, `
    INSERT INTO users_bio_log (user_id, bio, log_date)
    VALUES ($1, $2, now())`, userID, bio)
if err != nil {
    utilcommon.LogErr(ctx, err)
    return false
}

// Logları elasticsearch'e insert etme, dataları ES'e yolladığımızda oluşturduğu cluster içindeki node'larda tutuyor
log := map[string]interface{}{
    "user_id":  userID,
    "bio":      bio,
    "log_date": time.Now().Format(time.RFC3339),
}

_, err = c.elasticClient.Index().
    Index("changed_bio").
    BodyJson(log).
    Do(ctx)
if err != nil {
    utilcommon.LogErr(ctx, err)
    return false
}

return true
```

### Veri Aktarım Süreci
Burada postgreSQL'de bulunan dataların aktarıldığını düşündüğümüz bir senaryo üzerinden ilerliyoruz. Alternatif olarak, Kafka üzerinden gelen biyografi değişim logları doğrudan Elasticsearch’e aktarılabilir.\
Veya bir ETL (Extract, Transform, Load) script yazarak periyoduk olarak ES'e ekleyebiliriz
PostgreSQL tablosundaki veriler yine Elastic Stack bir tool olan Logstash ile postgresql'den datayı çekip elasticsearch'e aktarmak için kullanılabilir.

> Elasticsearch'e ait logstash ile data transfer örneği
```conf
input {
  jdbc {
    jdbc_driver_library => "/path/to/postgresql.jar"  # PostgreSQL driver
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://localhost:5432/your_database"
    jdbc_user => "your_username"
    jdbc_password => "your_password"
    schedule => "* * * * *"  # Her dakika çalıştır
    statement => "SELECT user_id, bio, log_date FROM users_bio_log WHERE log_date > now() - interval '1 day'"
  }
}

filter {
  mutate {
    rename => { "log_date" => "@timestamp" } # log_date alanını Elasticsearch'ün zaman tipi olan @timestamp ile eşleştirir.
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "user_bio_logs" # gönderilecek index
    document_id => "%{user_id}-%{log_date}" # Her kayda benzersiz bir document_id tanımlanır
  }

  stdout {
    codec => rubydebug  # Debug amaçlı output
  }
}
```

### ES'in Search Route'a Implementasyonu

Elasticsearch’i mevcut arama routelarına entegre etmek için önce Elasticsearch client’ı projeye eklenmeli ve PostgreSQL query'leri Elasticsearch query formatına dönüştürülürmeli

> Basit route implementasyonu
```go
return func(c *fiber.Ctx) error {
		term := c.Query("term")
		if len(term) < 3 {
			return util.ReturnErrorMessage(c, 400, "Bad request.")
		}

		ctx := util.FromCtx(c)
		resp := Response{
			[]modelcommon.UserData{},
		}

		// Elasticsearch Query
		esQuery := map[string]interface{}{
			"query": map[string]interface{}{
				"match": map[string]interface{}{
					"name": term,
				},
			},
		}
    res, err := r.elasticsearchClient.Search(
    			r.elasticsearchClient.Search.WithContext(ctx),
    			r.elasticsearchClient.Search.WithIndex("users"),
    			r.elasticsearchClient.Search.WithBodyString(string(queryJSON)),
    		)
    		if err != nil {
    			return util.ReturnErrorMessage(c, 500, "Elasticsearch search failed.")
    		}
    		defer res.Body.Close()
    	var esResult struct {
        			Hits struct {
        				Hits []struct {
        					Source modelcommon.UserData `json:"_source"`
        				} `json:"hits"`
        			} `json:"hits"`
        		}

        		if err := json.NewDecoder(res.Body).Decode(&esResult); err != nil {
        			return util.ReturnErrorMessage(c, 500, "Failed to parse Elasticsearch response.")
        		}

        		for _, hit := range esResult.Hits.Hits {
        			resp.Items = append(resp.Items, hit.Source)
        		}

        return c.JSON(resp)

```

. Verilerin düzenli olarak Elasticsearch’e indexlenmesi için bir  servisi yazılmalı (kullanılmalı). Sonuçların Elasticsearch üzerinden döneceği yeni bir refactor planlaması ile search endpoint’in performansı optimize edilebilir.

